{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "choice-check",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Ref\" data-toc-modified-id=\"Ref-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Ref</a></span></li><li><span><a href=\"#likelihood\" data-toc-modified-id=\"likelihood-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>likelihood</a></span></li><li><span><a href=\"#Maximum-likelihood\" data-toc-modified-id=\"Maximum-likelihood-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Maximum likelihood</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Summary</a></span></li><li><span><a href=\"#Scipy\" data-toc-modified-id=\"Scipy-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Scipy</a></span></li><li><span><a href=\"#Tensorflow-probability\" data-toc-modified-id=\"Tensorflow-probability-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Tensorflow-probability</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-weapon",
   "metadata": {},
   "source": [
    "# Ref\n",
    "\n",
    "* 世界第一簡單貝氏統計學\n",
    "* 林軒田 - L10 Logistic Regression - the likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-navigator",
   "metadata": {},
   "source": [
    "# likelihood\n",
    "\n",
    "又翻譯做 概度 or 可能性\n",
    "\n",
    "舉一個極簡單的例子，有一個抽獎箱，裡面有三種球，分別代表三種獎項\n",
    "\n",
    "舉例一個已知的生成機制(通常在真實場景下，我們是不知道母體分佈的)\n",
    "\n",
    "抽到 A 球的機率是 $\\frac{1}{4}$\n",
    "抽到 B 球的機率是 $\\frac{2}{4}$\n",
    "抽到 C 球的機率是 $\\frac{1}{4}$\n",
    "\n",
    "小明抽了6次，抽到的獎依序為\n",
    "\n",
    "BCABAA\n",
    "\n",
    "如果想要從小明抽到的獎推論A, B, C 球的抽中機率，該如何估計?\n",
    "\n",
    "Naive way(直接平均就對了?) :\n",
    "\n",
    "A : $\\frac{3}{6}$\n",
    "\n",
    "B : $\\frac{2}{6}$\n",
    "\n",
    "C : $\\frac{1}{6}$\n",
    "\n",
    "likelihood :\n",
    "\n",
    "數學上可以做一些假設，例如\n",
    "\n",
    "1. 每次抽球之間的行為是不相關的\n",
    "2. 球每次抽出來之後，同樣的球抽出的機率還是維持一定(例如我抽了A球，但因為箱子裡面A球太多，所以下一次抽到A球的機率還是一樣的)\n",
    "\n",
    "那麼小明抽到獎項 \n",
    "\n",
    "BCABAA的機率就稱作為 likelihood (或稱概度、可能性)\n",
    "\n",
    "抽中A球的機率 : $q_{A}$\n",
    "抽中B球的機率 : $q_{B}$\n",
    "抽中C球的機率 : $1 - q_{A} - q_{B}$\n",
    "\n",
    "而背後的生成機制如何形成這個可能性?\n",
    "\n",
    "likelihood :\n",
    "\n",
    "$$\n",
    "ll(q_{A}, q_{B}) = q_{B}^{2}q_{A}^{3}q_{C}\n",
    "$$\n",
    "\n",
    "# Maximum likelihood\n",
    "\n",
    "如果想要反推生成機制($q_{A}, q_{B}$)，其中一個觀點就是\n",
    "\n",
    "怎麼樣的$q_{A}, q_{B}$，最有可能產生小明這個抽獎結果? - 這個思路就是 Maximum likelihood\n",
    "\n",
    "\n",
    "用數學語言描述 : 你有一個多項式，有兩個變數，你想知道哪一組參數可以使得你的函數值最大\n",
    "\n",
    "$$\n",
    "max_{q_{A}, q_{B}} ll(q_{A}, q_{B})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{dll}{dq_{A}} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{dll}{dq_{B}} = 0\n",
    "$$\n",
    "\n",
    "\n",
    "由於 log 是單調函數\n",
    "\n",
    "因此找maximum，取log還是找得到(不會變成找minimum)\n",
    "\n",
    "$$\n",
    "max_{q_{A}, q_{B}} ll(q_{A}, q_{B}) ～ max log~ll(q_{A}, q_{B})\n",
    "$$\n",
    "\n",
    "$\\frac{dll}{dq_{A}} = \\frac{d (3log~q_{A} + 2log~q_{B} + log~(1 - q_{A} - q_{B}))}{q_{A}} = 0$\n",
    "\n",
    "$\\frac{dll}{dq_{A}} = \\frac{d (3log~q_{A} + 2log~q_{B} + log~(1 - q_{A} - q_{B}))}{q_{B}} = 0$\n",
    "\n",
    "\n",
    "連立方程式\n",
    "\n",
    "解出\n",
    "\n",
    "$q_{A} = \\frac{3}{6}$\n",
    "\n",
    "$q_{B} = \\frac{2}{6}$\n",
    "\n",
    "$q_{C} = \\frac{1}{6}$\n",
    "\n",
    "和人類直覺相符\n",
    "\n",
    "你可能會想說為何數字和真實生成機制\n",
    "\n",
    "抽到 A 球的機率是 $\\frac{1}{4}$\n",
    "抽到 B 球的機率是 $\\frac{2}{4}$\n",
    "抽到 C 球的機率是 $\\frac{1}{4}$\n",
    "\n",
    "\n",
    "不同?\n",
    "\n",
    "只是因為試驗的次數太少 =)\n",
    "\n",
    "# Summary\n",
    "\n",
    "likelihood : 從生成機制經過 i.i.d. 假設，把每個資料點的行程的機率都用符號表示，然後全部連乘，表示了生成機制(machenism)產生你看到的資料(data) 的機率，這條方程式就稱為 likelihood (或稱概率、可能性)\n",
    "\n",
    "maximum likelihood : 生成機制會是某些參數的函數(這會取決於我們建立模型的角度)，這些參數個別是多少的情況下，生成機制的概率(生成我手中資料地機率會最大?)，就是從這個角度去找參數\n",
    "\n",
    "1. 微分 - 當likelihood的數學形式具有解析解\n",
    "2. Autograd(Gradient-Descent-based) - 當likelihood的數學形式不具有解析解，就用數值解的方式\n",
    "3. Maximum Expectation - 其中一種 optimization，用在離散變數很好用\n",
    "\n",
    "...\n",
    "\n",
    "最佳化方法很多，看你的library support 到什麼程度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-softball",
   "metadata": {},
   "source": [
    "# Scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-grant",
   "metadata": {},
   "source": [
    "# Tensorflow-probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-conditioning",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light",
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.11.4"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
